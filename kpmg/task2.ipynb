{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Targeting high value customers based on customer demographics and attributes.\n",
    "> This project was done under the umbrella of KPMG internship experience. I was provided data sets of an organization targeting a client who wants a feedback from us on their dataset quality and how this can be improved.\n",
    "\n",
    "### Background\n",
    "- Sprocket Central Pty Ltd, a medium size bikes & cycling accessories organisation\n",
    "- needs help with its customer and transactions data\n",
    "- how to analyse it to help optimise its marketing strategy effectively.\n",
    "\n",
    "### Datasets\n",
    "- New Customer List\n",
    "- Customer Demographic\n",
    "- Customer Addresses\n",
    "- Transactions data in the past 3 months\n",
    "\n",
    "\n",
    "### Task\n",
    "- Exploratory Data Analysis to understand the data and its quality\n",
    "- Model building to predict the high value customers\n",
    "- Results and recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data\n",
    "xls = pd.ExcelFile('KPMG_VI_New_raw_data_update_final.xlsx')\n",
    "\n",
    "Transactions = pd.read_excel(xls, 'Transactions', skiprows=1)\n",
    "NewCustomerList = pd.read_excel(xls, 'NewCustomerList', skiprows=1)\n",
    "Demographic = pd.read_excel(xls, 'CustomerDemographic', skiprows=1)\n",
    "Address = pd.read_excel(xls, 'CustomerAddress', skiprows=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking correlation and common columns among the sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making variables to store the columns of each dataframe\n",
    "\n",
    "transactions_columns = Transactions.columns\n",
    "demographic_columns = Demographic.columns\n",
    "newcustomerlist_columns = NewCustomerList.columns\n",
    "address_columns = Address.columns\n",
    "\n",
    "transactions_columns.name = 'transactions_columns'\n",
    "demographic_columns.name = 'demographic_columns'\n",
    "newcustomerlist_columns.name = 'newcustomerlist_columns'\n",
    "address_columns.name = 'address_columns'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A code I prompted to generate a dataframe to generate a sheet vs column presence table\n",
    "# This shows if a column header is present across multiple dataframes or not\n",
    "\n",
    "def generate_presence_dataframe(*columns):\n",
    "    \"\"\"\n",
    "    Generate a DataFrame to show the presence of attributes in each column.\n",
    "\n",
    "    Args:\n",
    "        *columns: Variable number of pandas DataFrame columns.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with the presence of attributes in each column.\n",
    "        The column headers are based on the names of the passed columns,\n",
    "        or generic names if the columns don't have names.\n",
    "        The displayed columns are in the same order of the passed column arguments.\n",
    "        The DataFrame is sorted based on the number of '1' values horizontally (across the rows).\n",
    "\n",
    "    \"\"\"\n",
    "    # Step 1: Convert the column(s) to set(s)\n",
    "    column_sets = [set(col) for col in columns]\n",
    "\n",
    "    # Step 2: Create a set of all unique attributes from the column(s)\n",
    "    all_attributes = sorted(list(set().union(*column_sets)))\n",
    "\n",
    "    # Step 3: Create a dictionary to store the presence of attributes in each column\n",
    "    presence_dict = {'Attributes': all_attributes}\n",
    "    for i, col in enumerate(columns):\n",
    "        column_name = col.name if col.name else f'Column {i+1}'\n",
    "        presence_dict[column_name] = [1 if attr in col else 0 for attr in all_attributes]\n",
    "\n",
    "    # Step 4: Create a DataFrame from the presence dictionary\n",
    "    presence_df = pd.DataFrame(presence_dict)\n",
    "\n",
    "    # Step 5: Sort the dataframe based on the number of '1' values horizontally (across the rows)\n",
    "    presence_df = presence_df.iloc[presence_df.iloc[:, 1:].sum(axis=1).sort_values(ascending=False).index]\n",
    "\n",
    "    # Reset the index\n",
    "    presence_df = presence_df.reset_index(drop=True)\n",
    "\n",
    "    return presence_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_presence_df = generate_presence_dataframe(transactions_columns, demographic_columns, address_columns,newcustomerlist_columns)\n",
    "columns_presence_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highlights of the corelation analysis\n",
    "We are supposed to combine the data from the three sheets (Customer Demographic, Customer Addresses, Transactions) and then make a Master Sheet which would be a training set where we would train our model to predict the high value customers.\n",
    "- can use **customer_id** as the primary key to combine the data\n",
    "- there are some irrelevant columns in plain sight which can be dropped\n",
    "- there are rows with missing data which can be dropped as well, since it would affect the model building\n",
    "- the DOB can be converted to age, and then we can perform an analysis with different age groups\n",
    "- a **polynomial regression** model can be used to predict the high value customers\n",
    "- values for the states, DOB and gender can be made uniform."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a master Dataframe to train the model\n",
    "- The master dataframe is built by combining the three sheets using the customer_id as the primary key\n",
    "- The irrelevant columns are dropped\n",
    "- reference for [pandas merge](https://www.youtube.com/watch?v=h4hOPGo4UVU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the dataframes using customer_id as the key\n",
    "merged_df = pd.merge(Demographic, Address, on='customer_id', how='outer')\n",
    "master_df = pd.merge(merged_df, Transactions, on='customer_id', how='outer')\n",
    "master_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = ['transaction_id', 'product_id', 'first_name', 'last_name', 'default', 'country']\n",
    "# Create a new DataFrame by dropping the specified columns\n",
    "master_stripped = master_df.drop(columns=columns_to_drop)\n",
    "master_stripped.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".internship",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
